{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, Holiday,\n",
    "import numpy as np \n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarning(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm \n",
    "import matploatlib.pyplot as pyplot\n",
    "from statsmodels.tsa.statespace.sarimax import sarimax\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_absoluate_error, mean_absoluate_percentage_error\n",
    "from xgboost import xgbregressor\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters setup and create forecast DF\n",
    "train_start_date = '2020-01-03'\n",
    "train_end_date = '2023-12-24'\n",
    "\n",
    "future_prediction_start_date = '2023-12-31'\n",
    "future_prediction_end_date = '2024-12-19'\n",
    "\n",
    "fpsd = datetime.strptime(future_prediction_start_date, '%Y-%m-%d')\n",
    "fped = datetime.strptime(future_prediction_end_date, '%Y-%m-%d')\n",
    "\n",
    "#create prediction dataframe\n",
    "index_of_fc = pd.date_range(fpsd, periods = n_periods, freq = 'W-SUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import actual data and regressors actuals\n",
    "\n",
    "historical_data = []\n",
    "regressor_historicals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "def sample_data_split(data):\n",
    "\n",
    "    n_test = int(len(data)*0.2)\n",
    "    train = data[~data.index.isin(data.index.unique()[-n_test:])]\n",
    "    test = data[~data.index.isin(data.index.unique()[-n_test:])]\n",
    "\n",
    "    return n_test,train.sort_values(by = 'ship_week'),test.sort_values(by='ship_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up sarima and xgboost models\n",
    "def sarima_model(train,test,exog_df_fut,ar,i,ma,ar_s,i_s,ma_s,s):\n",
    "    exog_df_future = exog_df_fut.copy()\n",
    "    model =sarimax(train['var'],exog = train.iloc[:,1:],order=(ar,i,ma),seasonal_order=(ar_s,i_s,ma_s.s),enforce_invertibility=False)\n",
    "    model_fit = model.fit(maxiter=1000,method='nm')\n",
    "    print(model_fit.pvalues[model_pvalues<0.05].index)\n",
    "\n",
    "    train['pred'] = model_fit.get_prediction(start = train.index[0], end = train.index[len(train)-1],\\\n",
    "                                              index = train.index\\\n",
    "                                              exog = pd.dataframe(train.iloc[:,1:]), \\\n",
    "                                              dynamic = False).predicted_mean\n",
    "                                \n",
    "    test['pred'] = model_fit.get_prediction(start = test.index[0], end = test.index[len(test)-1],\\\n",
    "                                            exog = pd.dataframe(test.iloc[:,1:]),\\\n",
    "                                            dynamic = False).predicted_mean\n",
    "\n",
    "    future_df['pred'] = pred_series[test.shape[0]:]\n",
    "    test_mape = mean_absolute_percentage_error(test['var'],test.pred)\n",
    "    test_mae = mean_absolute_error(test['var'],test.pred)\n",
    "\n",
    "    return (train,test,future_df,test_mape,test_mae,model)\n",
    "\n",
    "def auto_arima_model(train_df,test_df,exog_df_fut,p_list,d_list,q_list,P_list,D_list,Q_list):\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    exog_df_future = exog_df_fut.copy()\n",
    "    a_df = pd.concat([train,test])\n",
    "    model_fit = auto_arima(a_df['var'],X=a_df.iloc[:,1:],\n",
    "                           start_p=p_list[0],max_p=p_list[1],max_d=d_list[0],\n",
    "                           start_q=q_list[0],max_q=q_list[1],\n",
    "                           start_P=P_list[0],max_P=P_list[1],max_D=D_list[0],\n",
    "                           seasonal = True,\n",
    "                           start_Q=Q_list[0],max_Q=Q_list[1],\n",
    "                           maxiter=2000,scoring='mae',\n",
    "                           out_of_sample_size=len(test)\n",
    "                           )\n",
    "    print(model_fit.summary())\n",
    "    \n",
    "    train['pred'],confint_train = model_fit.predict(start = a_df.index[0],end = a_df.index[len(a_df)-len(test)-1], \\\n",
    "                                                    n_periods=len(a_df)-len(test),X = a_df.iloc[:-len(test),1:],return_conf_int=True)\n",
    "    train['confint_low'] = pd.series(confint_train[:,0], index=test.index)\n",
    "    train['confint_high'] = pd.series(confint_train[:,1], index=test.index)\n",
    "    \n",
    "\n",
    "    test['pred'],confint_test = model_fit.predict(start = test.index[0], end = test.index[len(test)-1],\\\n",
    "                                                  n_periods=len(test),X = test.iloc[:,1:],return_conf_int=True)\n",
    "    test['confint_low'] = pd.series(confint_test[:,0], index=test.index)\n",
    "    test['confint_high'] = pd.series(confint_test[:,1], index=test.index)\n",
    "\n",
    "    test_mae = mean_absolute_error(test['var'],test.pred)\n",
    "    test_mape = mean_absolute_percentage_error(test['var'],test.pred)\n",
    "\n",
    "    future_df = exog_df_future.copy()\n",
    "    future_df['pred'],confint_fut = model_fit.predict(start = future_df.index[0], end = future_df.index[len(future_df)-1],\\\n",
    "                                                      n_peirods=len(future_df),X = future_df,return_conf_int = True)\n",
    "    future_df['confint_low'] = pd.series(confint_fut[:, 0], index=future_df.index)\n",
    "    future_df['confint_high'] = pd.series(confint_fut[:, 1], index=future_df.index)\n",
    "\n",
    "    return (train, test, future_df, test_mape, test_mae, model_fit)\n",
    "\n",
    "def fit_sarima_model(a_d, exog_df_fut):\n",
    "    exog_df_future = exog_df_fut.copy()\n",
    "    X = a_df['var'].values\n",
    "    result = adfuller(X,autolag='AIC', regression='ct')\n",
    "    print('ADF Statistic: %f' % results[0])\n",
    "    print('p-value: %f' % results[1])\n",
    "    print(\"Num Of Lags : \",% results[2])\n",
    "    print('Critical Values:')\n",
    "    \n",
    "    for key, value in results[4].items():\n",
    "        print('\\t%s: %.3f' % (key,value))\n",
    "\n",
    "    is_valid = 1\n",
    "    if results[1] > 0.1\n",
    "        print('Time series is not stationary, SARIMA is not recommended.')\n",
    "        is_valid = 0\n",
    "\n",
    "    n_sarima, train_df, test_df = sample_data_split(a_df)\n",
    "    var_stat_mean = a_df['var'].mean()\n",
    "    var_stat_std = a_df['var'].std()\n",
    "\n",
    "    if var_stat_std !=0:\n",
    "        sm.graphics.tsa.plot_acf(a_df['var'])\n",
    "        plt.show()\n",
    "        sm.graphics.tsa.plot_pacf(a_df['var'])\n",
    "        plt.show()\n",
    "\n",
    "        decompose_data = seasonal_decompose(a_df['var'], model=\"additive\")\n",
    "        decompose_data.plot()\n",
    "\n",
    "        p_list = [0:15]\n",
    "        d_list = [15]\n",
    "        q_list = [0,15]\n",
    "        P_list = [0:15]\n",
    "        D_list = [15]\n",
    "        Q_list = [0,15]\n",
    "\n",
    "        train_df_aa, test_df_aa, fuuture_df_aa, test_mape_aa, test_mae_aa, model_aa = auto_arima_model(train_df,test_df,exog_df_future,p_list,\\\n",
    "                                                                                                     d_list,q_list_,P_list,D_list,Q_list)\n",
    "\n",
    "        print('Auto Arima Model Error, MAPE', test_mape_aa,', MAE:',test_mae_aa)\n",
    "\n",
    "        train_df_sa, test_df_sa, future_df_sa, test_mape_sa, test_mae_sa, model_sa = sarima_model(train_df,test_df,exog_df_future,0,1,0,0,0,0,52)\n",
    "        print('SARIMAX Model Error, MAPE', test_mape_sa,', MAE:', test_mae_sa)\n",
    "\n",
    "        if (test_mae_aa <= test_mae_sa):\n",
    "            test_arima_pred = test_df_aa.copy()\n",
    "            train_arima_pred = train_df_aa.copy()\n",
    "            future_df_pred = future_df_aa.copy()\n",
    "            best_model_mae = test_mae_aa\n",
    "        else:\n",
    "            test_arima_pred = test_df_aa.copy()\n",
    "            train_arima_pred = train_df_aa.copy()\n",
    "            future_df_pred = future_df_aa.copy()\n",
    "            best_model_mae = test_mae_sa\n",
    "\n",
    "    else: \n",
    "        train_arima_pred = train_df.copy()\n",
    "        test_arima_pred = test_df.copy()\n",
    "        train_arima_pred['pred'] = var_stat_mean\n",
    "        test_arima_pred['pred'] = var_stat_mean\n",
    "        best_model_mae = 0\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(a_df[\"var\"], color='#1f76b4', label='actual')\n",
    "    plt.plot(pd.concat([train_df_aa['pred'],test_df_aa['pred'],future_df_aa['pred']]), color='darkgreen' ,label=\"autoarima\")\n",
    "    plt.plot(pd.concat([train_df_sa['pred'],test_df_sa['pred'],future_df_sa['pred']]), color='red' ,label=\"sarima\")\n",
    "    plt.fill_between(pd.concat([a_df,future_df_aa]).index,\n",
    "                    pd.concat([train_df_aa['confint_low'],test_df_aa['confint_ dlow'],future_df_aa['confint_low']]),\n",
    "                    pd.concat([train_df_aa['confint_high'],test_df_aa['confint_high'],future_df_aa['confint_high']]),\n",
    "                    color='k',alpha=.15,label=\"autoarima conf\")\n",
    "\n",
    "    return train_arima_pred,test_arima_pred,future_df_pred,is_valid,best_model_mae\n",
    "\n",
    "\n",
    "def add_lags(df)\n",
    "    target_map = df['var'].to_dict()\n",
    "    df['lag1'] = (df.index - pd.Timedeltaa(53,\"W\")).map(target_map)\n",
    "    return df\n",
    "\n",
    "def add_lags_future(a_old_df,a_future_df):\n",
    "    a_date = (a_future_df.index - pd.Timedelta(53,\"W\"))\n",
    "    a_old_var = a_old_df.loc[a_date,'var']\n",
    "    a_future_df['lag1'] = a_old_var.values\n",
    "    return a_future_df\n",
    "\n",
    "\n",
    "def xgboost_model(train_data,test_data,exog_df_future_xgb_data):\n",
    "    exog_df_future_xgb = exog_df_future_xgb_data.copy()\n",
    "    train_data_x = train_data.iloc[:,1:]\n",
    "    train_data_y = train_data['var']\n",
    "\n",
    "    test_data_x = test_data.iloc[:,1:]\n",
    "    test_data_y = test_data[['var']]\n",
    "\n",
    "    model = XBGRegressor(objective='reg:absoluteerror',n_estimators=1000)\n",
    "    #define model evaluation method\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    #evaluate model\n",
    "    scores - cross_val_score(model, train_data_x, train_data_y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    #force scores to be positive\n",
    "    scores = np.absolute(scores)\n",
    "    print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()))\n",
    "\n",
    "    model.fit(train_data_x,train_data_y)\n",
    "    test_data_y['pred'] = model.predict(test_data_x)\n",
    "    test_data['pred'] = model.predict(exog_df_future_xgb)\n",
    "\n",
    "    test_mape = mean_absolute_percentage_error(test_data_y['var'],test_data_y.pred)\n",
    "    test_mae = mean_absolute_error(test_data_y['var'],test_data_y.pred)\n",
    "\n",
    "    return train_data,test_data,exog_df_future_xgb,test_mape,test_mae,model\n",
    "\n",
    "def fit_xgboost_model(a_df,exog_df_fut):\n",
    "    exog_df_futre = exog_df_fut.copy()\n",
    "    xgboost_a_df = add_lags(a_df)\n",
    "\n",
    "    exog_df_fut_xgb = add_lags_future(a_df,exog_df_future)\n",
    "\n",
    "    n_sarima, train_df, test_df = sample_data_split(a_df)\n",
    "\n",
    "    var_stat_mean = a_df['var'].mean()\n",
    "    var_stat_std = a_df['var'].mean()\n",
    "\n",
    "    if var_stat_std!=0:\n",
    "        train_df_xg, test_df_xg, future_df_xg, test_mape_xg, test_mae_xg,model = xgboost_model(train_df,test_df,exog_df_fut_xgb)\n",
    "        print('XGBoost Model Error, MAPE', test_mape_xg,', MAE:',test_mae_xg)\n",
    "        best_model_mae = test_mae_xg\n",
    "    else:\n",
    "        train_df_xg = train_df.copy()\n",
    "        test_df_xg = test_df.copy()\n",
    "        future_df_xg = exog_df_future.copy() \n",
    "        train_df_xg['pred'] = var_stat_mean\n",
    "        test_df_xg['pred'] = var_stat_mean\n",
    "        future_df_xg['pred'] = var_stat_mean\n",
    "        best_model_mae = 0\n",
    "    \n",
    "    plt.plot(pd.concat([train_df_xg['pred'],test_df_xg['pred'],future_df_xg['pred']]),color='black',label=\"xgb\")\n",
    "\n",
    "    return train_df_xg, test_df_xg,future_df_xg,best_model_mae\n",
    "\n",
    "def plot_actual_pred(a_df,pred_col,act_col):\n",
    "\n",
    "    ax, fig = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    plt.plot(a_df[a_df['type']=='Train'][act_col], label=\"Actual Train\")\n",
    "    plt.plot(a_df[a_df['type']=='Train'][pred_col],ls=\"--\", label=\"Prediction Train\")\n",
    "    \n",
    "    plt.plot(a_df[a_df['type']=='Test'][act_col], label=\"Actual Test\")\n",
    "    plt.plot(a_df[a_df['type']=='Test'][pred_col],ls=\"--\", label=\"Prediction Test\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xticks(alpha=0.75, weight=\"hold\")\n",
    "    plt.yticks(alpha=0.75, weight=\"hold\")\n",
    "\n",
    "    plt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\n",
    "    plt.ylabel(pred_col,alpha=0.75, weight=\"bold\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def pct_of_total(a_df,a_agg_list,a_col_metric):\n",
    "    a_df_agg = a_df.groupby(a_agg_list).sum()[a_col_metric].reset_index()\n",
    "    a_df_agg.rename(columns={a_col_metric:a_col_metric+'_total'},inplace=True)\n",
    "\n",
    "    a_df = a_df.merge(a_df_agg,on=a_agg_list)\n",
    "    a_df['final '+a_col_metric] = a_df[a_col_metric]*1.0/a_df[a_col_metric+'_total']\n",
    "\n",
    "    a_df.drop(columns=[a_col_metric,a_col_metric+'_total'],inplace=True)\n",
    "    a_df.rename(columns={'final '+a_col_metric:a_col_metric},inplace=True)\n",
    "\n",
    "    a_df[a_col_metric] = a_df[a_col_metric]*100.0\n",
    "\n",
    "    return a_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "def ensemble_model(trasin_df_arima, train_df_xgb, test_df_arima, test_df_xgb, future_df_xg,is_valid,model_mae_arima,model_mae_xgb):\n",
    "    #preprocess train dataset\n",
    "    train_df_arima.rename(columns={'pred':'arima_pred'},inplace=True)\n",
    "    train_df_xgb.rename(columns={'pred':'xgb_pred'},inplace=True)\n",
    "    train_df_final = train_df_arima.copy()\n",
    "    train_df_final['is_arima_valid'] = is_valid\n",
    "    train_df_final['xgb_pred'] = train_df_xgb['xgb_pred']\n",
    "\n",
    "    #preprocess test dataset\n",
    "    test_df_arima.rename(columns={'pred':'arima_pred'},inplace=True)\n",
    "    test_df_xgb.rename(columns={'pred':'xgb_pred'},inplace=True)\n",
    "    test_df_final = test_df_arima.copy()\n",
    "    test_df_final['is_arima_valid'] = is_valid\n",
    "    test_df_final['xgb_pred'] = test_df_xgb['xgb_pred']\n",
    "\n",
    "    #preprocess forecasting dataset\n",
    "    future_df_arima.rename(columns={'pred':'arima_pred'},inplace=True)\n",
    "    future_df_xgb.rename(columns={'pred':'xgb_pred'},inplace=True)\n",
    "    future_df_final = future_df_arima.copy()\n",
    "    future_df_final['is_arima_valid'] = is_valid\n",
    "    future_df_final['xgb_pred'] = future_df_xgb['xgb_pred']\n",
    "\n",
    "    #if\n",
    "    if (model_mae_xgb <= model_mae_arima):\n",
    "        train_df_final['best_pred'] = train_df_final['xgb_pred']\n",
    "        test_df_final['best_pred'] = test_df_final['xgb_pred']\n",
    "        future_df_final['best_pred'] = test_df_final['xgb_pred']\n",
    "        best_mae = model_mae_xgb\n",
    "        model_use = 'XGB'\n",
    "    else:        \n",
    "        train_df_final['best_pred'] = train_df_final['arima_pred']\n",
    "        test_df_final['best_pred'] = test_df_final['arima_pred']\n",
    "        future_df_final['best_pred'] = test_df_final['arima_pred']\n",
    "        best_mae = model_mae_xgb\n",
    "        model_use = 'ARIMA'\n",
    "\n",
    "    train_df_final['model_used'] = model_used\n",
    "    test_df_final['model_used'] = model_used\n",
    "    future_df_final['model_used'] = model_used\n",
    "    print('Best MAE: ',best_mae)\n",
    "    \n",
    "    return train_df_final,test_df_final,future_df_final,best mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topline = historical_cube_data_df.groupby['ship_week'].sum()['shipments'].reset_index()\n",
    "topline.rename(columns={'shipments':'topline'},inplace=True)\n",
    "topline = pd.concat([topline,topline_future_op2]).copy()\n",
    "\n",
    "carrier_mix = historical_cube_data_df.groupby(['ship_week','carrier']).sum()['shipments'].reset_index()\n",
    "\n",
    "carrier_mix = pct_of_total(carrier_mix,['ship_week'],'shipments')\n",
    "carrier_mix.rename(column={'shipments':'carrier_mix'},inplace=True)\n",
    "carrier_mix = pd.concat([carrier_mix,carrier_mix_future_op2]).copy()\n",
    "carrier_mix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculating network cube per package & network upb\n",
    "\n",
    "cpp_upb = historical_cube_data_df.groupby(['ship_week']).sum()[['units','shipments','cube']].reset_index()\n",
    "cpp_upb['upb'] = cpp_upb['units']*1.0/cpp_upb['shipments']\n",
    "cpp_upb['cpp'] = cpp_upb['cube']*1.0/cpp_upb['shipments']\n",
    "cpp_upb.drop(columns=['units','shipments','cube'],inplace=True)\n",
    "cpp_upb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_create_holiday_df(a_df):\n",
    "    holiday_df = a_df.merge(holiday_list_df,how='left',left_on='ship_week',\\\n",
    "                            right_on='week_start').drop(columns=['week_start'])\n",
    "    holiday_df['event_name'].fillna('NA',inplace=True)\n",
    "    dummy_cols = pd.get_dummies(list(holiday_df['event_name'])).drop(columns='NA')\n",
    "\n",
    "    holiday_df = pd.concat([holiday_df,dummy_cols],axis=1).drop(columns=['event_name'])\n",
    "\n",
    "    holiday_df = holiday_df.merge(peak_dates_df,how='left',left_on='ship_week',right_on='week_start'\\\n",
    "                                ).drop(columns=['week_start'])\n",
    "    holiday_df['is_peak'].fillna(0,inplace=True)\n",
    "    return holiday_df\n",
    "\n",
    "holiday_df = f_create_holiday_df(historical_cube_data_df[['ship_week']].drop_duplicates())\n",
    "\n",
    "holiday_df_future = f_create_holiday_df(pd.dataframe(index=index_of_fc\\\n",
    "                                                    ).reset_index().rename(columns={'index':'ship_week'}))\n",
    "\n",
    "exog_df_fut = holiday_df_future.merge(cpp_upb_future,how='left',on='ship_week').set_index('ship_week')\n",
    "exog_df_fut.index.freq = 'W-Sun'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing old carriers which are avaiable in training dataset\n",
    "removed_carrier_list = ['carrier_d']\n",
    "\n",
    "carrier_size_cat_df = historical_cube_data_df[~historical_cube_data_df['carrier'].isin(removed_carrier_list)\\\n",
    "                      ][['carrier','pkg_size_group']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "all_weeks = pd.dataframe(historical_cube_data_df['ship_week'].unique(),columns=['ship_week']\\\n",
    "                        ).sort_values(by = 'ship_week')\n",
    "\n",
    "for index, row in carrier_size_cat_df.iterrows():\n",
    "    a_carrier = row['carrier']\n",
    "    carrier_df = historical_cube_data_df[historical_cube_data_df['carrier']==a_carrier\\\n",
    "                                            ].groupby(['ship_week','pkg_size_group']\\\n",
    "                                                    ).sum()['shipments'].reset_index()\n",
    "\n",
    "    carrier_df_total = carrier_df.groupby('ship_week').sum()['shipments'].reset_index().rename(columns={'shipments':'tot_shipments'\\\n",
    "                                                                                            })\n",
    "\n",
    "    carrier_size_mix_df=carrier_df.merge(carrier_df_total, how='left',on='ship_week')\n",
    "    carrier_size_mix_df['size_mix'] = carrier_size_mix_df['shipments']*100.0/carrier_size_mix_df['tot_shipments']\n",
    "\n",
    "    a_size_category = row['pkg_size_group']\n",
    "    carrier_size_df = carrier_size_mix_df[carrier_size_mix_df['pkg_size_group']==a_size_category\\\n",
    "                                         ].drop(columns=['pkg_size_group','shipments','tot_shipments']\\\n",
    "                                         ).reset_index(drop=True)\n",
    "\n",
    "    carrier_size_df = all_weeks.merge(carrier_df_total,how='left',on='ship_week')\n",
    "    carrier_size_df['size_mix'].fillna(0,inplace=True)\n",
    "\n",
    "    print('Training ',a_carrier,'-',a_size_category, ' model')\n",
    "    var_stat_mean, var_stat_std = carrier_size_df['size_mix'].mean(),carrier_size_df['size_mix'].std()\n",
    "    print('Mean ',var_stat_mean)\n",
    "    print('Standard Deviation ',var_stat_std)\n",
    "\n",
    "    #adding holiday df\n",
    "    carrier_size_df = carrier_size_df.merge(holiday_df,how='left',on='ship_week')\n",
    "\n",
    "    #adding network upb&cpp\n",
    "    carrier_size_df = carrier_size_df.merge(cpp_upb,how='left',on='ship_week')\n",
    "    carrier_size_df.set_index('ship_week',inplace=True)\n",
    "    carrier_size_df.rename(columns={'size_mix':'var'},inplace=True)\n",
    "\n",
    "    ##sarima model/auto arima\n",
    "    train_df_arima, test_df_arima, future_df_arima, is_valid, model_mae_arima = fit_sarima_model(carrier_size_df,exog_df_fut)\n",
    "    #print(1/0)\n",
    "\n",
    "    ##xgboost model \n",
    "    train_df_xgb, test_df_xgb, future_df_xg, model_mae_xgb = fit_xgboost_model(carrier_size_df,exog_df_fut)\n",
    "    plt.title(a_carrier+'-'+a_size_category)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    ##ensemble model\n",
    "    train_df_ens,test_df_ens,future_df_ens,model_mae_ens = ensemble_model(train_df_arima, train_df_xgb,\\\n",
    "                                                                          test_df_arima,test_df_xgb,\\\n",
    "                                                                          future_df_arima,future_df_xg,\\\n",
    "                                                                          is_valid,model_mae_arima,model_mae_xgb)\n",
    "\n",
    "    train_df_ens['type'] = 'Train'\n",
    "    test_df_ens['type'] = 'Test'\n",
    "    future_df_ens['type'] = 'Future'\n",
    "\n",
    "    final_df = pd.concat([train_df_ens,test_df_ens,future_df_ens])\n",
    "\n",
    "    final_df['carrier'] = a_carrier\n",
    "    final_df['size_cat'] = a_size_category\n",
    "    final_df['test_mae'] = model_mae_ens\n",
    "\n",
    "    if index ==0:\n",
    "        predicted_df = final_df.copy()\n",
    "    else:\n",
    "        predicted_df = pd.concat([predicted_df,final_df]) \n",
    "\n",
    "    print(a_carrier,'-',a_size_category,' model MAE:',model_mae_ens)\n",
    "\n",
    "predicted_df['model'] = 'Size_Model'\n",
    "\n",
    "predicted_df['xgb_pred'] = np.maximum(0,predicted_df['xgb_pred'])\n",
    "predicted_df['best_pred'] = np.maximum(0,predicted_df['best_pred'])\n",
    "predicted_df['arima_pred'] = np.maximum(0,predicted_df['arima_pred'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing old carriers which are avaiable in training dataset\n",
    "removed_carrier_list = ['carrier_d']\n",
    "\n",
    "carrier_size_cpp_df = historical_cube_data_df[~historical_cube_data_df['carrier'].isin(removed_carrier_list)\\\n",
    "                      ][['carrier','pkg_size']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "for index, row in carrier_size_cpp_df.iterrows():\n",
    "    a_carrier = row['carrier']\n",
    "    a_size_category = row['pkg_size']\n",
    "    carrier_df = historical_cube_data_df[(historical_cube_data_df['carrier']==a_carrier)&\\\n",
    "                                         (historical_cube_data_df['pkg_size']==a_size_category)].copy()\n",
    "    \n",
    "    carrier_size_df = carrier_df.groupby('ship_week').sum()[['cube','shipments']].reset_index()\n",
    "\n",
    "    carrier_size_df['cupp'] = carrier_size_df['cube']*1.0/carrier_size_df['shipments']\n",
    "    carrier_size_df.drop(columns=['cube','shipments'],inplace=True)\n",
    "    carrier_size_df = all_weeks.merge(carrier_size_df,how='left',on='ship_week')\n",
    "    carrier_size_df['cupp'].fillna(0,inplace=True)\n",
    "\n",
    "    print('Training ',a_carrier,'-',a_size_category,' model')\n",
    "\n",
    "    var_stat_mean,var_stat_std = carrier_size_df['cupp'],mean(),carrier_size_df['cupp'].std()\n",
    "    print('Mean ',var_stat_mean)\n",
    "    print('Standard Deviation ',var_stat_std)\n",
    "\n",
    "\n",
    "    #adding holiday df\n",
    "    carrier_size_df = carrier_size_df.merge(holiday_df,how='left',on='ship_week')\n",
    "\n",
    "    #adding network upb&cpp\n",
    "    carrier_size_df = carrier_size_df.merge(cpp_upb,how='left',on='ship_week')\n",
    "    carrier_size_df.set_index('ship_week',inplace=True)\n",
    "    carrier_size_df.rename(columns={'cupp':'var'},inplace=True)\n",
    "\n",
    "    ##sarima model/auto arima\n",
    "    train_df_arima, test_df_arima, future_df_arima, is_valid, model_mae_arima = fit_sarima_model(carrier_size_df,exog_df_fut)\n",
    "    #print(1/0)\n",
    "\n",
    "    ##xgboost model \n",
    "    train_df_xgb, test_df_xgb, future_df_xg, model_mae_xgb = fit_xgboost_model(carrier_size_df,exog_df_fut)\n",
    "    plt.title(a_carrier+'-'+a_size_category)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    ##ensemble model\n",
    "    train_df_ens,test_df_ens,future_df_ens,model_mae_ens = ensemble_model(train_df_arima, train_df_xgb,\\\n",
    "                                                                          test_df_arima,test_df_xgb,\\\n",
    "                                                                          future_df_arima,future_df_xg,\\\n",
    "                                                                          is_valid,model_mae_arima,model_mae_xgb)\n",
    "\n",
    "    train_df_ens['type'] = 'Train'\n",
    "    test_df_ens['type'] = 'Test'\n",
    "    future_df_ens['type'] = 'Future'\n",
    "\n",
    "    final_df = pd.concat([train_df_ens,test_df_ens,future_df_ens])\n",
    "\n",
    "    final_df['carrier'] = a_carrier\n",
    "    final_df['size_cat'] = a_size_category\n",
    "    final_df['test_mae'] = model_mae_ens\n",
    "\n",
    "    if index ==0:\n",
    "        predicted_df = final_df.copy()\n",
    "    else:\n",
    "        predicted_df = pd.concat([predicted_df,final_df]) \n",
    "\n",
    "    print(a_carrier,'-',a_size_category,' model MAE:',model_mae_ens)\n",
    "\n",
    "predicted_df['model'] = 'Size_cpp_Model'\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
